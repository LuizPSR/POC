{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4eb379-66be-4227-b7b2-217d9838e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               text language\n",
      "0       Hello world       en\n",
      "1  Bonjour le monde       fr\n",
      "2        Hola mundo       es\n",
      "3        Hallo Welt       it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'text': ['Hello world', 'Bonjour le monde', 'Hola mundo', 'Hallo Welt']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang, _ = langid.classify(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['language'] = df['text'].apply(detect_language)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b720961-03d4-47ef-bea0-b18515584069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{A: 200140*D**2/(B*C*D**2 + 2*B*C - 202*B - 282*C + D**2 + 28482)}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "# Define symbols\n",
    "A, B, C, D = symbols('A B C D', integer=True, positive=True)\n",
    "\n",
    "# Given total parameters\n",
    "total_params = 200142\n",
    "\n",
    "# Dimensions of the input image\n",
    "H, W = 140, 100\n",
    "\n",
    "# Formulas for H', W', and total parameters\n",
    "H_prime = H - B + 1\n",
    "W_prime = W - C + 1\n",
    "\n",
    "# Total parameters equation\n",
    "conv_params = (B * C + 1) * A\n",
    "pool_output = (H_prime * W_prime) / D**2 * A * 2 + 2\n",
    "total_equation = Eq(conv_params + pool_output, total_params)\n",
    "\n",
    "# Solve for plausible values of A, B, C, and D\n",
    "solution = solve(total_equation, (A, B, C, D), dict=True)\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1563a4c-0b26-49f1-a90c-64c7909c9ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Apply the function to the DataFrame\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(remove_stopwords))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the necessary data for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'text': ['Hello world, this is a test.', \n",
    "                 'Bonjour le monde, ceci est un test.', \n",
    "                 'Hola mundo, esto es una prueba.', \n",
    "                 'Hallo Welt, das ist ein Test.']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define stopwords (English in this example)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "\n",
    "\n",
    "print(df['text'].values.map(remove_stopwords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
