{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7fd8574-c3cb-466d-aa2d-0475e3805c1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d2e098-6a6c-487f-afd3-9866d3a03b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# data manipulation and normalization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Neural Network utilits\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# set gpu if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"gpu\")\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090a06ad-5fd4-468f-8329-071dd0f6e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\S'\n",
      "/tmp/ipykernel_7627/1279606008.py:12: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  MENTION_PATTERN = \"@\\w+\"\n",
      "/tmp/ipykernel_7627/1279606008.py:13: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  LINK_PATTERN = \"http\\S+|www\\.\\S+\"\n"
     ]
    }
   ],
   "source": [
    "# Global Variables\n",
    "\n",
    "INPUT_SIZE = 140         # max tokens allowed in a text\n",
    "EMBEDDING_SIZE = 100     # dimensions used by the embedding layer\n",
    "TARGET_SIZE = 2          # features in the dataset\n",
    "MAX_VOCABULARY = 10 ** 5 # max number of tokens possible\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH  = 32\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "MENTION_PATTERN = \"@\\w+\"\n",
    "LINK_PATTERN = \"http\\S+|www\\.\\S+\"\n",
    "EMOJI_PATTERN = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" #emotions\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" #sumbols and pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" #transport and map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" #flags\n",
    "                           u\"\\U00002702-\\U000027B0\"  \n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\",flags = re.UNICODE)\n",
    "\n",
    "DATA_SET = \"Sentiment140.csv\" # path and headers of the dataset\n",
    "HEADERS = [ 'target', 'ids', 'date', 'flag', 'user', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f8e1d-ee19-4f47-b5ae-691303083960",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Seting Determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38019ef9-cfcd-44aa-9e30-dfaade941457",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed950521-6add-4c0a-895d-a2d0adcf0997",
   "metadata": {},
   "source": [
    "# Explore and Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ba97a-eeac-4980-964e-2ad30f8802ea",
   "metadata": {},
   "source": [
    "We are using the [Sentimental 140](https://www.kaggle.com/datasets/kazanova/sentiment140) database, that contains the following 6 fields:\n",
    "\n",
    "**target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive). No neutral entry is present.\n",
    "\n",
    "**ids**: The id of the tweet ( 2087 )\n",
    "\n",
    "**date**: the date of the tweet ( Sat May 16 23:58:44 UTC 2009 )\n",
    "\n",
    "**flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "**user**: the user that tweeted (robotickilldozr)\n",
    "\n",
    "**text**: the text of the tweet (Lyx is cool)\n",
    "\n",
    "Since we only care about the sentiment of the text, only the text and target fields will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7336c16-0b90-4b69-9cd3-2f83495ffb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_SET, names=HEADERS, encoding = \"latin\", )\n",
    "df = df[['target', 'text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb6bc3-f912-4ff3-b430-3c48f2d1fbbe",
   "metadata": {},
   "source": [
    "Now that we narrow down the useful columns, lets normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f675e95-349a-4038-94b2-69b06fa70a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe values =  [0 4]\n",
      "normalized values =  [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"dataframe values = \", df['target'].unique())\n",
    "df.loc[df['target'] == 0, 'target'] = 0\n",
    "df.loc[df['target'] == 4, 'target'] = 1\n",
    "print(\"normalized values = \", df['target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142cd69-4587-4a7c-93d4-98cada7880aa",
   "metadata": {},
   "source": [
    "For the text, the normalization will include the following steps:\n",
    "1. remove mentions\n",
    "2. remove special characters (flags, emojis, etc)\n",
    "3. remove links\n",
    "4. remove punctuation\n",
    "5. set to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d25c95c-82ce-4587-950a-0166cde59b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww thats a bummer you shoulda got david carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  awww thats a bummer you shoulda got david carr...\n",
       "1       0  is upset that he cant update his facebook by t...\n",
       "2       0  i dived many times for the ball managed to sav...\n",
       "3       0     my whole body feels itchy and like its on fire\n",
       "4       0  no its not behaving at all im mad why am i her..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \n",
    "    # remove metions\n",
    "    text = re.sub(MENTION_PATTERN, \"\", text)\n",
    "    # remove special symbols\n",
    "    text = re.sub(EMOJI_PATTERN, \"\", text)\n",
    "    # remove links\n",
    "    text = re.sub(LINK_PATTERN, \"\", text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'))\n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "    \n",
    "df['text'] = df['text'].map(normalize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ced3ad-eeb4-449a-b4b3-f5591c007563",
   "metadata": {},
   "source": [
    "Lets also mix it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd533cb8-0df4-4315-8929-15ad62f11726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1)\n",
    "df['target'].values[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfda0e0-6745-4412-ad0a-e0f3f71ef66a",
   "metadata": {},
   "source": [
    "Now we tokenize the text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91574189-5d2d-4c3c-881c-c29834c6c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1600000, 140]) torch.Size([1600000])\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and vectorization\n",
    "def tokenize_and_vectorize(texts, max_vocab_size, input_size):\n",
    "    # Create a vocabulary dictionary (word -> index)\n",
    "    word_counts = Counter(\" \".join(texts).split())\n",
    "    most_common = word_counts.most_common(max_vocab_size-1) # include empty word\n",
    "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(most_common)}  # start index at 1\n",
    "\n",
    "    # Convert texts to sequences of word indices\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequence = [vocab.get(word, 0) for word in text.split()]  # 0 for unknown words\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Padding sequences\n",
    "    padded_sequences = [seq[:input_size] + [0] * (input_size - len(seq)) if len(seq) < input_size else seq[:input_size] for seq in sequences]\n",
    "    return np.array(padded_sequences), vocab\n",
    "\n",
    "# Tokenize and vectorize\n",
    "X, vocab = tokenize_and_vectorize(df['text'], MAX_VOCABULARY, INPUT_SIZE)\n",
    "y = df['target'].values\n",
    "\n",
    "# Convert X and y to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Print dimensions\n",
    "print(X_tensor.shape, y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999d519b-02a9-482a-8a44-a6c3e410ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we already shuffled the data, lets just split it on data loaders\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        X_tensor[:int(-len(X_tensor)*VAL_SPLIT)], \n",
    "        y_tensor[:int(-len(X_tensor)*VAL_SPLIT)]), \n",
    "    batch_size=BATCH)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        X_tensor[int(-len(X_tensor)*VAL_SPLIT):], \n",
    "        y_tensor[int(-len(X_tensor)*VAL_SPLIT):]), \n",
    "    batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c11c62-8af2-4555-9bce-20d646408344",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multiple Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16618d17-085e-4524-92d6-663116d1c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model (same as before)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embedding = nn.Embedding(MAX_VOCABULARY, EMBEDDING_SIZE)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(INPUT_SIZE * EMBEDDING_SIZE, TARGET_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee353315-3792-403c-8665-229bb4146020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Embedding: 1-1                         10,000,000\n",
       "├─Flatten: 1-2                           --\n",
       "├─Linear: 1-3                            28,002\n",
       "=================================================================\n",
       "Total params: 10,028,002\n",
       "Trainable params: 10,028,002\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(MLP())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca402e65-023f-4ac6-8227-96c5ab165026",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378d1171-c23e-441c-b8ab-7e2fe4b59be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(MAX_VOCABULARY, EMBEDDING_SIZE)\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=1, \n",
    "            out_channels=20, \n",
    "            kernel_size=(100, 100))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(20, 20), stride=1)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(22, TARGET_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        #print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(x.shape)\n",
    "        x = self.conv(x).squeeze(3)\n",
    "        #print(x.shape)\n",
    "        #x = torch.permute(x,(0,3,1,2))\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = F.softmax(self.fc(x), dim=1)  # Output layer with softmax for probabilities\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2cb739f-5eee-4556-bb3a-91c37e2cd714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CNN                                      --\n",
       "├─Embedding: 1-1                         10,000,000\n",
       "├─Conv2d: 1-2                            200,020\n",
       "├─MaxPool2d: 1-3                         --\n",
       "├─Flatten: 1-4                           --\n",
       "├─Linear: 1-5                            46\n",
       "=================================================================\n",
       "Total params: 10,200,066\n",
       "Trainable params: 10,200,066\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(CNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d481c-5038-4ab1-9c70-cc2d1da9916d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Long-Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928900a7-7211-492f-b972-aa6c01b6c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(MAX_VOCABULARY, EMBEDDING_SIZE)  # Embedding layer\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size=EMBEDDING_SIZE, \n",
    "                            hidden_size=128,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * 128, TARGET_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_length, embedding_size]\n",
    "\n",
    "        # Staked LSTM layers\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # Shape: [batch_size, seq_length, 2 * hidden_size]\n",
    "\n",
    "        # Use the last hidden state for classification\n",
    "        x = torch.cat((h_n[-2], h_n[-1]), dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.softmax(self.fc(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76b7a5c7-99c7-4168-9a8f-991aeb48d10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LSTM                                     --\n",
       "├─Embedding: 1-1                         10,000,000\n",
       "├─LSTM: 1-2                              630,784\n",
       "├─Linear: 1-3                            514\n",
       "=================================================================\n",
       "Total params: 10,631,298\n",
       "Trainable params: 10,631,298\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(LSTM())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f62a65-fb88-4751-a4c6-c359156432bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Swarm Characteristic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3efe415-cc6e-4b7b-a33b-a5529cc43149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SwarmFilter(nn.Module):\n",
    "    def __init__(self, units=32):\n",
    "        super(SwarmFilter, self).__init__()\n",
    "        self.units = units\n",
    "        self.filter = nn.Parameter(torch.randn(units))  # Trainable parameter with random initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the mean along the last axis, keeping the dimensions\n",
    "        mean_values = torch.mean(x, dim=-1, keepdim=True)\n",
    "        return mean_values * self.filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d30a1d5-8a4b-47a1-850f-b4ecb0ac8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(MAX_VOCABULARY, EMBEDDING_SIZE)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.swarm1 = SwarmFilter(units=300)\n",
    "        self.swarm2 = SwarmFilter(units=10)\n",
    "        self.fc = nn.Linear(10, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = self.flatten(x)     \n",
    "        x = self.swarm1(x)     \n",
    "        x = self.swarm2(x)     \n",
    "        x = self.fc(x)         \n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aebff43-0fc7-4e50-8766-cdb8677e91ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SCNN                                     --\n",
       "├─Embedding: 1-1                         10,000,000\n",
       "├─Flatten: 1-2                           --\n",
       "├─SwarmFilter: 1-3                       300\n",
       "├─SwarmFilter: 1-4                       10\n",
       "├─Linear: 1-5                            22\n",
       "=================================================================\n",
       "Total params: 10,000,332\n",
       "Trainable params: 10,000,332\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(SCNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77319416-9e43-49b2-81e6-1d5b77efa72f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "810a7b2d-7a57-41d0-9945-76ba56ea39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, epochs=EPOCHS):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        \n",
    "        for _, data in tqdm.tqdm(enumerate(train_loader, 0), unit=\"batch\", total=len(train_loader)):\n",
    "            inputs, labels = data\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "    \n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Calculate accuracy (for tracking)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "            # Track the running loss\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "        # Print training stats for the epoch\n",
    "        print(f\"\\tTraining Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
    "    \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "    \n",
    "        with torch.no_grad():  # Disable gradient calculation during validation\n",
    "            for inputs, labels in valid_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "    \n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "    \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "                # Track validation loss\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "        # Print validation stats\n",
    "        print(f\"\\tValidation Loss: {val_loss/len(valid_loader):.4f}, Accuracy: {100 * val_correct/val_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9b031-817d-477a-abc2-d1887b012db2",
   "metadata": {},
   "source": [
    "# Comparative Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8af03a7f-8175-4c82-bc19-2e35ce5698ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:53<00:00, 259.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.5483, Accuracy: 75.41%\n",
      "\tValidation Loss: 0.5120, Accuracy: 77.57%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:55<00:00, 256.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4803, Accuracy: 79.19%\n",
      "\tValidation Loss: 0.5268, Accuracy: 77.56%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:56<00:00, 255.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4436, Accuracy: 81.08%\n",
      "\tValidation Loss: 0.5592, Accuracy: 76.96%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:56<00:00, 254.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4088, Accuracy: 82.82%\n",
      "\tValidation Loss: 0.6023, Accuracy: 76.28%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:57<00:00, 253.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.3760, Accuracy: 84.42%\n",
      "\tValidation Loss: 0.6517, Accuracy: 75.69%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:56<00:00, 254.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.3464, Accuracy: 85.83%\n",
      "\tValidation Loss: 0.7060, Accuracy: 75.07%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:55<00:00, 256.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.3203, Accuracy: 87.07%\n",
      "\tValidation Loss: 0.7655, Accuracy: 74.60%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:56<00:00, 255.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.2976, Accuracy: 88.10%\n",
      "\tValidation Loss: 0.8277, Accuracy: 74.05%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:56<00:00, 255.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.2780, Accuracy: 89.01%\n",
      "\tValidation Loss: 0.8898, Accuracy: 73.64%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [02:55<00:00, 256.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.2612, Accuracy: 89.76%\n",
      "\tValidation Loss: 0.9526, Accuracy: 73.23%\n"
     ]
    }
   ],
   "source": [
    "run_model(MLP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9985b9c-823e-4bdc-8205-0676d4647212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:14<00:00, 26.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.5375, Accuracy: 76.35%\n",
      "\tValidation Loss: 0.5196, Accuracy: 78.54%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:14<00:00, 26.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.5081, Accuracy: 79.72%\n",
      "\tValidation Loss: 0.5123, Accuracy: 79.32%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:14<00:00, 26.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.5002, Accuracy: 80.60%\n",
      "\tValidation Loss: 0.5150, Accuracy: 79.17%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:15<00:00, 26.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4941, Accuracy: 81.29%\n",
      "\tValidation Loss: 0.5157, Accuracy: 79.12%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:15<00:00, 26.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4888, Accuracy: 81.89%\n",
      "\tValidation Loss: 0.5133, Accuracy: 79.38%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:15<00:00, 26.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4841, Accuracy: 82.41%\n",
      "\tValidation Loss: 0.5132, Accuracy: 79.43%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:15<00:00, 26.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4798, Accuracy: 82.90%\n",
      "\tValidation Loss: 0.5130, Accuracy: 79.47%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:15<00:00, 26.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4760, Accuracy: 83.33%\n",
      "\tValidation Loss: 0.5138, Accuracy: 79.42%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:17<00:00, 26.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4726, Accuracy: 83.70%\n",
      "\tValidation Loss: 0.5142, Accuracy: 79.37%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [28:16<00:00, 26.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4696, Accuracy: 84.03%\n",
      "\tValidation Loss: 0.5151, Accuracy: 79.35%\n"
     ]
    }
   ],
   "source": [
    "run_model(CNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52193b96-fa40-4851-882a-102d77fc5009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:58<00:00, 68.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4999, Accuracy: 80.10%\n",
      "\tValidation Loss: 0.4822, Accuracy: 82.03%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:56<00:00, 68.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4712, Accuracy: 83.33%\n",
      "\tValidation Loss: 0.4782, Accuracy: 82.57%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:56<00:00, 68.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4599, Accuracy: 84.63%\n",
      "\tValidation Loss: 0.4782, Accuracy: 82.62%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:56<00:00, 68.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4527, Accuracy: 85.47%\n",
      "\tValidation Loss: 0.4788, Accuracy: 82.62%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:56<00:00, 68.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4476, Accuracy: 86.06%\n",
      "\tValidation Loss: 0.4789, Accuracy: 82.66%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:56<00:00, 68.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4445, Accuracy: 86.40%\n",
      "\tValidation Loss: 0.4798, Accuracy: 82.62%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:57<00:00, 68.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4424, Accuracy: 86.63%\n",
      "\tValidation Loss: 0.4802, Accuracy: 82.59%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:57<00:00, 68.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4404, Accuracy: 86.85%\n",
      "\tValidation Loss: 0.4812, Accuracy: 82.47%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:57<00:00, 68.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4393, Accuracy: 86.96%\n",
      "\tValidation Loss: 0.4807, Accuracy: 82.52%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 45000/45000 [10:57<00:00, 68.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4384, Accuracy: 87.07%\n",
      "\tValidation Loss: 0.4810, Accuracy: 82.51%\n"
     ]
    }
   ],
   "source": [
    "run_model(LSTM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "781bc300-0e6a-4997-a879-be5dc3a8dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:11<00:00, 234.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.5267, Accuracy: 77.28%\n",
      "\tValidation Loss: 0.5086, Accuracy: 79.62%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 236.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.5028, Accuracy: 80.21%\n",
      "\tValidation Loss: 0.5061, Accuracy: 79.88%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:11<00:00, 235.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4982, Accuracy: 80.72%\n",
      "\tValidation Loss: 0.5064, Accuracy: 79.84%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 236.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4954, Accuracy: 81.04%\n",
      "\tValidation Loss: 0.5054, Accuracy: 79.94%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 236.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4933, Accuracy: 81.27%\n",
      "\tValidation Loss: 0.5044, Accuracy: 80.05%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 236.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4918, Accuracy: 81.46%\n",
      "\tValidation Loss: 0.5040, Accuracy: 80.07%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:13<00:00, 232.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4905, Accuracy: 81.61%\n",
      "\tValidation Loss: 0.5038, Accuracy: 80.12%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 235.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4893, Accuracy: 81.74%\n",
      "\tValidation Loss: 0.5036, Accuracy: 80.16%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 235.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4884, Accuracy: 81.84%\n",
      "\tValidation Loss: 0.5037, Accuracy: 80.17%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 45000/45000 [03:10<00:00, 236.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Loss: 0.4875, Accuracy: 81.94%\n",
      "\tValidation Loss: 0.5039, Accuracy: 80.14%\n"
     ]
    }
   ],
   "source": [
    "run_model(SCNN())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
