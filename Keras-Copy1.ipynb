{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2e098-6a6c-487f-afd3-9866d3a03b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation and normalization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Neural Network utilits\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Embedding,     \n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Layer,\n",
    "    \n",
    "    Conv2D, \n",
    "    MaxPooling2D, \n",
    "    Reshape,\n",
    "    \n",
    "    Bidirectional,\n",
    "    LSTM\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.ops import mean, outer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a06ad-5fd4-468f-8329-071dd0f6e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "INPUT_SIZE = 140         # max tokens allowed in a text\n",
    "EMBEDDING_SIZE = 100     # dimensions used by the embedding layer\n",
    "TARGET_SIZE = 2          # features in the dataset\n",
    "MAX_VOCABULARY = 10 ** 5 # max number of tokens possible\n",
    "\n",
    "OPTIMIZER = Adam(learning_rate=0.001)\n",
    "EPOCHS = 10\n",
    "BATCH  = 32\n",
    "VALIDATION_PLIT = 0.1\n",
    "\n",
    "MENTION_PATTERN = \"@\\w+\"\n",
    "LINK_PATTERN = \"http\\S+|www\\.\\S+\"\n",
    "EMOJI_PATTERN = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" #emotions\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" #sumbols and pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" #transport and map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" #flags\n",
    "                           u\"\\U00002702-\\U000027B0\"  \n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\",flags = re.UNICODE)\n",
    "\n",
    "DATA_SET = \"Sentiment140.csv\" # path and headers of the dataset\n",
    "HEADERS = [ 'target', 'ids', 'date', 'flag', 'user', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f8e1d-ee19-4f47-b5ae-691303083960",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Seting TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38019ef9-cfcd-44aa-9e30-dfaade941457",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed950521-6add-4c0a-895d-a2d0adcf0997",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Explore and Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ba97a-eeac-4980-964e-2ad30f8802ea",
   "metadata": {},
   "source": [
    "We are using the [Sentimental 140](https://www.kaggle.com/datasets/kazanova/sentiment140) database, that contains the following 6 fields:\n",
    "\n",
    "**target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive). No neutral entry is present.\n",
    "\n",
    "**ids**: The id of the tweet ( 2087 )\n",
    "\n",
    "**date**: the date of the tweet ( Sat May 16 23:58:44 UTC 2009 )\n",
    "\n",
    "**flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "**user**: the user that tweeted (robotickilldozr)\n",
    "\n",
    "**text**: the text of the tweet (Lyx is cool)\n",
    "\n",
    "Since we only care about the sentiment of the text, only the text and target fields will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7336c16-0b90-4b69-9cd3-2f83495ffb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_SET, names=HEADERS, encoding = \"latin\", )\n",
    "df = df[['target', 'text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb6bc3-f912-4ff3-b430-3c48f2d1fbbe",
   "metadata": {},
   "source": [
    "Now that we narrow down the useful columns, lets normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f675e95-349a-4038-94b2-69b06fa70a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe values =  [0 4]\n",
      "normalized values =  [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"dataframe values = \", df['target'].unique())\n",
    "df.loc[df['target'] == 0, 'target'] = 0\n",
    "df.loc[df['target'] == 4, 'target'] = 1\n",
    "print(\"normalized values = \", df['target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142cd69-4587-4a7c-93d4-98cada7880aa",
   "metadata": {},
   "source": [
    "For the text, the normalization will include the following steps:\n",
    "1. remove mentions\n",
    "2. remove special characters (flags, emojis, etc)\n",
    "3. remove links\n",
    "4. remove punctuation\n",
    "5. set to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d25c95c-82ce-4587-950a-0166cde59b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww thats a bummer you shoulda got david carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  awww thats a bummer you shoulda got david carr...\n",
       "1       0  is upset that he cant update his facebook by t...\n",
       "2       0  i dived many times for the ball managed to sav...\n",
       "3       0     my whole body feels itchy and like its on fire\n",
       "4       0  no its not behaving at all im mad why am i her..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \n",
    "    # remove metions\n",
    "    text = re.sub(MENTION_PATTERN, \"\", text)\n",
    "    # remove special symbols\n",
    "    text = re.sub(EMOJI_PATTERN, \"\", text)\n",
    "    # remove links\n",
    "    text = re.sub(LINK_PATTERN, \"\", text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'))\n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "    \n",
    "df['text'] = df['text'].map(normalize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ced3ad-eeb4-449a-b4b3-f5591c007563",
   "metadata": {},
   "source": [
    "Lets also mix it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd533cb8-0df4-4315-8929-15ad62f11726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1)\n",
    "df['target'].values[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfda0e0-6745-4412-ad0a-e0f3f71ef66a",
   "metadata": {},
   "source": [
    "Now we tokenize the text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91574189-5d2d-4c3c-881c-c29834c6c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 140)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and vectorization\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCABULARY)       \n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text']) \n",
    "\n",
    "# Padding sequences\n",
    "padded = pad_sequences(sequences, maxlen=INPUT_SIZE, padding='post')\n",
    "\n",
    "# print dimensions\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa7011a-8448-442f-9685-3d1a419d809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded\n",
    "y = to_categorical(df['target'].values, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c11c62-8af2-4555-9bce-20d646408344",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multiple Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16618d17-085e-4524-92d6-663116d1c24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MLP\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MLP\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">28,002</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │      \u001b[38;5;34m10,000,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │          \u001b[38;5;34m28,002\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,028,002</span> (38.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,028,002\u001b[0m (38.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,028,002</span> (38.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,028,002\u001b[0m (38.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = Sequential([\n",
    "    Input((INPUT_SIZE,)),\n",
    "    Embedding(MAX_VOCABULARY, EMBEDDING_SIZE),\n",
    "    Flatten(),\n",
    "    Dense(units=TARGET_SIZE, activation='softmax'),\n",
    "], name=\"MLP\")\n",
    "mlp.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ac61e-9bb1-4b38-b2d2-db2f18d29bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m45000/45000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1252s\u001b[0m 28ms/step - accuracy: 0.7756 - loss: 0.4846 - val_accuracy: 0.7969 - val_loss: 0.4603\n",
      "Epoch 2/10\n",
      "\u001b[1m45000/45000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1201s\u001b[0m 27ms/step - accuracy: 0.8225 - loss: 0.4100 - val_accuracy: 0.7779 - val_loss: 0.5131\n",
      "Epoch 3/10\n",
      "\u001b[1m45000/45000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1201s\u001b[0m 27ms/step - accuracy: 0.8574 - loss: 0.3415 - val_accuracy: 0.7602 - val_loss: 0.5993\n",
      "Epoch 4/10\n",
      "\u001b[1m 1133/45000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19:22\u001b[0m 27ms/step - accuracy: 0.8645 - loss: 0.3263"
     ]
    }
   ],
   "source": [
    "mlp.fit(X, y, batch_size=BATCH, epochs=EPOCHS, validation_split=VALIDATION_PLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca402e65-023f-4ac6-8227-96c5ab165026",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57207b17-43b5-4236-9fd0-66d86963fde4",
   "metadata": {},
   "source": [
    "We must assume a mistype in the original paper, as it is not possible to transform the input (140, 100) passing by a convolutional layer with kernel = 100 x 100, them again with a maxpooling  20 x 20, due to dimensionality loss throught the kernels\n",
    "\n",
    "Thus we aproximate the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d1171-c23e-441c-b8ab-7e2fe4b59be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential([\n",
    "    Input((INPUT_SIZE,)),\n",
    "    Embedding(MAX_VOCABULARY, EMBEDDING_SIZE),\n",
    "\n",
    "    Reshape((140,100,1)),\n",
    "    Conv2D(filters=20, kernel_size=(100, 100), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(20, 20)),\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(TARGET_SIZE, activation='softmax'),\n",
    "], name=\"CNN\")\n",
    "cnn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb739f-5eee-4556-bb3a-91c37e2cd714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.fit(X, y, batch_size=BATCH, epochs=EPOCHS, validation_split=VALIDATION_PLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d481c-5038-4ab1-9c70-cc2d1da9916d",
   "metadata": {},
   "source": [
    "# Long-Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928900a7-7211-492f-b972-aa6c01b6c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential([\n",
    "    Input((INPUT_SIZE,)),\n",
    "    Embedding(MAX_VOCABULARY, EMBEDDING_SIZE),\n",
    "\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Bidirectional(LSTM(128)),\n",
    "\n",
    "    \n",
    "    Dense(424, activation='relu'),\n",
    "    Dense(units=2, activation='softmax'),\n",
    "], name=\"LSTM\")\n",
    "lstm.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7a5c7-99c7-4168-9a8f-991aeb48d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.fit(X, y, batch_size=BATCH, epochs=EPOCHS, validation_split=VALIDATION_PLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f62a65-fb88-4751-a4c6-c359156432bf",
   "metadata": {},
   "source": [
    "# Swarm Characteristic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efe415-cc6e-4b7b-a33b-a5529cc43149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwarmFeature(Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.filter = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return mean(inputs, axis=-1, keepdims=True) * self.filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30a1d5-8a4b-47a1-850f-b4ecb0ac8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scnn = Sequential([\n",
    "    Input((INPUT_SIZE,)),\n",
    "    Embedding(MAX_VOCABULARY, EMBEDDING_SIZE),\n",
    "    \n",
    "    Flatten(),\n",
    "    SwarmFeature(units=300),\n",
    "    SwarmFeature(units=10),\n",
    "    \n",
    "    Dense(units=2, activation='softmax'),\n",
    "], name=\"SCNN\")\n",
    "scnn.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "scnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b040b21-b893-4fd8-992e-7c24bc43651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scnn.fit(X, y, batch_size=BATCH, epochs=EPOCHS, validation_split=VALIDATION_PLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9b031-817d-477a-abc2-d1887b012db2",
   "metadata": {},
   "source": [
    "# Comparative Analyse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
